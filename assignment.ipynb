{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment Group 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nMZZV417dWs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiDn2Sk-VWqE",
        "outputId": "ee99a5ac-5b5a-471c-ea0c-7115f60896dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/annawalstra/tm10007_Group5.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK9m4YpjyWu6",
        "colab_type": "text"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NE_fTbKGe5z",
        "outputId": "d3549380-fea1-4ed6-af67-00a124976dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# General packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection \n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn import decomposition\n",
        "# Too learn more about metrics see: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "\n",
        "# Data loading functions. Uncomment the one you want to use\n",
        "from adni.load_data import load_data\n",
        "data = load_data()\n",
        "print(f'The number of spamples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n",
        "# Creating X and Y \n",
        "X = data.loc[:, data.columns != 'label']\n",
        "Y = data['label']\n",
        "\n",
        "# Preprocessing: deleting features with only zeros\n",
        "X = X.loc[:, (X != 0).any(axis=0)]\n",
        "# print(f'The number of spamples: {len(X.index)}')\n",
        "# print(f'The number of columns: {len(X.columns)}')\n",
        "\n",
        "# Binarize Y labels\n",
        "y_bin = preprocessing.label_binarize(Y, ['CN','AD'])\n",
        "y_bin = [i[0] for i in y_bin]\n",
        "#print(y_bin)\n",
        "\n",
        "# Split the dataset in train and test part\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, y_bin, test_size=0.5, stratify=y_bin)\n",
        "\n",
        "\n",
        "# Scale the data \n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "#Fitting the PCA algorithm with our Data\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA().fit(X_train_scaled)\n",
        "#Plotting the Cumulative Summation of the Explained Variance\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Variance (%)') #for each component\n",
        "plt.title('Dataset Explained Variance')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Perform a PCA\n",
        "pca = decomposition.PCA(n_components=50)\n",
        "pca.fit(X_train_scaled)\n",
        "X_train_pca = pca.transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The number of spamples: 855\n",
            "The number of columns: 268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd5xddZ3/8dd7ZjKZlElCOukhJEAo\nQoyAgBRBaVJ0LYDIgiysCqKu+hMXC+rq6rLWFUVQmjTBVYxKWxSkKJDQkhBaCKmQnkwmmUz//P44\nZ+DOMC3J3Lkzc97Px+M+7rmnfr5zk/O53+/3nO9RRGBmZtlVVOgAzMyssJwIzMwyzonAzCzjnAjM\nzDLOicDMLOOcCMzMMs6JwKwTJB0taWUn1/2opPvyFMeDkv4lH/tu5VhXSfpqdxzLCsuJwJC0VNJ2\nSZWSNkv6u6RPSOrUvw9JUySFpJI8x9nhcSRdLqlO0tac1+Z8xtVSRNwcEe/tzmNKOiP9HtVifomk\ntZLet6P7jIhPRMS3ui5K66mcCKzJKRFRDkwGvgt8CfhVYUPaab+JiME5r2GFDqgb3AkMA45qMf8E\nIIB7dmRnkoq7KC7rBZwIrJmIqIiIOcBHgH+WtB+ApJMlPS1pi6QVki7P2eyh9H1z+gv8nZKmSfqr\npA2S1ku6WdIbJ2RJX5K0Kq2FvCjp2HR+kaRLJb2Sbnu7pOFtHWdHyibpsDSWiennt0naJGnv9PNS\nSV+WtCidf52ksjb21RRjZbr++3OWnSvpkZzPkdawXk5rXFfm/nKX9HFJz6fHvFfS5Jxl75H0gqQK\nST8Fmv3ibxIR1cDtwDktFp0D3BIR9ZLukLQ63ddDkvbNOc71kn4u6S5J24Bj0nn/kS7fTdKfJK1L\n4/yTpAk52z8o6VuSHk3/JvdJGpmz/Ii0prk5/fdzbjq/v6T/lrRc0pq0OWpAm1+i5YUTgbUqIp4A\nVgLvSmdtIzmpDANOBj4p6fR02ZHp+7D0F/g/SE5Y/wmMA/YBJgKXA0jaC7gYeEdaCzkeWJru49PA\n6SS/bMcBm4Ar2znOjpTp78AvgBvSk81NwFcj4oWc1T6axjMNmAF8pY3dvULytxkKfAO4SdLu7Rz+\nfcA7gAOAD6fHQNJpwL8DHwBGAQ8Dt6bLRgK/S2MYmR7z8HaOcQPwwaYTqaShwCnpfIC7genAaOAp\n4OYW258FfBsoBx5psawIuI6kxjgJ2A78tJXtz0v3Xwp8IY1jcnrs/0nLeCDwTLrNd0n+zgcCewLj\nga+1U0bLh4jwK+MvkpPwca3Mfwy4rI1tfgT8MJ2eQtL8UNLOMU4Hnk6n9wTWAscB/Vqs9zxwbM7n\n3YE6oKSTx7kcqAU257weyFneD3gSWEDSXKIWf4dP5Hw+CXglnT4aWNnOcZ8BTkunzwUeyVkWwBE5\nn28HLk2n7wbOz1lWBFSRnHDPAR7LWSaS5Pwv7cTxMnBWOn0B8Gwb6w1L4xqafr4euLHFOtcD/9HG\n9gcCm3I+Pwh8Jefzp4B70ukvA79vZR8i+YExLWfeO4FXC/1/Imsv1wisPeOBjQCSDpH0QNo0UAF8\nguRXaqskjZF0W9r8s4Xk1/dIgIhYDHyW5KS9Nl1vXLrpZOD3aRPCZpLE0ACM2YG4b4+IYTmvY5oW\nREQdyQluP+D7kZ59cqzImV5GUitprXznSHomJ879aOfvAazOma4CBqfTk4Ef5+xnI8kJcnx67Dfi\nSWPNja81N/Jm89DH0s9IKpb03bQ5awtv1sByY25z35IGSvqFpGXp9g8Bw9S8L6GtMk4kqc20NAoY\nCDyZU/570vnWjZwIrFWS3kFyMmpqIrgFmANMjIihwFW82V7d2hC230nn7x8RQ4Czc9YnIm6JiCNI\nToQBfC9dtAI4scWJvCwiVrVxnB0t13jg6yTNHN+X1L/FKhNzpicBr7Wyj8nANSTNWyMi6YxeSBvt\n9x1YAfxri/IOiKQZ6/XceNJ+hYlt7Sj1a+DYtP/kUN5s/jkLOI2kFjaUpHZFi5jb+/t+HtgLOCT9\nPpua6TpT5hUkTW0trSdpYto3p+xDI2JwK+taHjkRWDOShii51PA24KaIWJAuKgc2RkS1pINJTixN\n1gGNwB4588qBrUBFevL9Ys4x9pL07vQkXE1yMmhMF18FfLupw1TSqLQdva3j7EjZRFIb+BVwPsmJ\ntuXlkRdJmqCkg/oy4Det7GoQyUlzXbrf80hqBDvjKuDLTR23koZK+lC67M/AvpI+oOSS2UuAse3t\nLCKWkiTvW4H/i4imX+nlQA2wgeRX+Hd2MM5yku9pc/q3+foObHszcJykDyu5nHWEpAMjopEkof5Q\n0mhIErWk43cwNttFTgTW5I+SKkl+vV0G/ICk46/Jp4Bvput8jaSdG4CIqCLpZHw0reIfStKBOguo\nIDmh/S5nX/1JOgnXkzQnjCZpRwb4MUnN4770WI8Bh7RznNZ8RM3vI9ianmguSY/11bSZ5TzgPEnv\nytn2FuA+YAlJc8Z/tNx5RCwCvg/8A1gD7A882kYs7YqI35PUhm5Lm1wWAiemy9YDHyL5W20g6ejt\nzHFuIKlp3Zgz70aSpq5VwCKSv+uO+BEwgOQ7e4wduBw1IpaT9Ld8nqTp6xngbeniLwGLgcfS8t9P\nUvOwbqS3NpGaZZOkpSQdsfcXOhaz7uQagZlZxjkRmJllnJuGzMwyzjUCM7OMy+tokfkwcuTImDJl\nSqHDMDPrVZ588sn1EdHqzXq9LhFMmTKFefPmFToMM7NeRdKytpa5acjMLOOcCMzMMs6JwMws45wI\nzMwyzonAzCzj8pYIJF2r5KHZC9tYLkk/kbRY0nxJs/IVi5mZtS2fNYLrSR6c3ZYTSUZTnA5cCPw8\nj7GYmVkb8nYfQUQ8JGlKO6ucRvJovCAZgnaYpN0j4vV8xWTWV0QEDY1BQ/pe3xg0tnhvaMxZFkF9\nQ/reGDQ0NtLQCPWNjURABDRGsjzS/Tc2Jg9daIxI1wkaA4L0PZ3fGM0/Ny1/y3ZvbN80nSx/o0xv\nKWPusmhzWWt/m7bWi2brdW7/bzlUAYflOXafMbxt4rAu328hbygbT/NH461M570lEUi6kKTWwKRJ\nk7olOLPWRAQ19Y1sr21ge136qm3+Xp07r66B6nS6riHZtq4hedWm07UNQW19srxpfm2zdYK6+kbq\nGhtpTE/ejR4irKC0M8+i6wKjh5T1uUTQaRFxNXA1wOzZs/1fwHZJRLClup6N22rZuK2WTdtq2VhV\ny+aqWiqr66msrmdLdV06XcfWmvo35ldW11HXsGP/BCUoKymmtKSIfsVF9C8pol+x6Fdc9Ma80uIi\nBvQrZkhZyRvzS4uL3pguSdcvLhLFUvKevkqKmn/OXaekWBRJlBQVUVwExUVFlBSJonS7IokiQVFR\n8g7pZwml75C8FxWB0uXKWS7eXL9pXsvPzdfJ2QfNT6pq8eTL9k64nd2u5S6Us1DN5re9Xl9XyESw\niubPX52QzjPbKfUNjazfWsvqLdWsrqhmzZbktTp9X1dZw8ZtdWyuqqW+jZ/UEgzuX8KQsn6Ul5VQ\nXlbC6PIypo0qYXD/EsrT+YNKixlQWkxZv2IG9EumB/RLP6fTTfP7lxRl6qRivU8hE8Ec4GJJt5E8\nirDC/QPWnohgU1UdyzdWJa8N21i+sYplG6pYsbGK1Vuq39JkUlIkRpf3Z8zQMqaMGMTbJ5ey28BS\nhg9KXrsNKmXEoGTesIH9GFRaQlGRT9qWLXlLBJJuBY4GRkpaSfKw634AEXEVcBfJc0wXA1U0fz6u\nZVhEsG5rDS+v2cpLayp5ee1WXl5TyUtrtlKxva7ZuqPL+zNp+EAOnTaC8cMGMGZIGWOHlDF2aBlj\nhpQxYlCpT+xmHcjnVUNndrA8gIvydXzrHRobg2Ubq1iwqoIFKzezYFUFL6yuZHPVmyf8oQP6MWPM\nYE4+YHf2GDmIySMGMXnEQCbuNpABpcUFjN6sb+gVncXWd6zZUs28pZt4ZsUmFqyq4LlVW6isqQeg\ntKSIfcaWc+J+Y5k+upwZY8qZMWYwo8r7u43dLI+cCCxvIoLFa7cyd+km5i3byLylm1i+sQpIT/q7\nD+G0g8ax//ih7Dd+KDPGlNOv2KOemHU3JwLrUqsrqnn45XU8sng9jy5ez/qttQCMHFzK7MnDOeed\nk3nHlOHMHDfEJ32zHsKJwHZJXUMjjy/ZyP3Pr+GRxetZvHYrkJz4D99zJIdNG8HBU0cwZcRAN++Y\n9VBOBLbDttc28NDL67h34Wr+8sJaKrbXUdaviIOnjuDDsydwxJ6j2Htsua/WMeslnAisU2rqG3jg\nhbXc+fRrPPjSWqrrGhlSVsJxM8dw/L5jOXL6KF/BY9ZLORFYmxobg3nLNvH7p1fy5/mvs6W6npGD\n+/Oht0/k+H3Hcsgew93Ob9YHOBHYW2zYWsMdT67klseXs3xjFQNLizlh37GcftB4Dps2ghKf/M36\nFCcCA5JLPect28RNjy3j7gWrqW1o5JCpw/nce6Zz/L5jGVjqfypmfZX/d2dcfUMjdy9czS8eeoWF\nq7ZQXlbCWYdM4qOHTGL6mPJCh2dm3cCJIKOqauu5Y95KfvnIElZs3M4eIwfxnffvz+kHjfOvf7OM\n8f/4jKmqrefGfyzj6oeWsHFbLbMmDeMrJ8/kPfuM8eWeZhnlRJAR22sbuPnxZfz8wVfYsK2Wo2aM\n4tPv3pPZU4YXOjQzKzAngj6uvqGRW59Yzk/+uph1lTW8a/pIPnvcDN4+ebdCh2ZmPYQTQR/291fW\n8405i3hxTSWHTB3OlWfN4uCprgGYWXNOBH3Qio1VfOeu57l74Wom7DaAq85+O8fvO8Zj/ZhZq5wI\n+pC6hkaueXgJP77/ZYokPv+eGVxw5B6U9fPQD2bWNieCPmLhqgr+32/ns+j1LZy431i++r6ZjBs2\noNBhmVkv4ETQy1XXNfDjv7zM1Q8tYfigUq46exYn7Ld7ocMys17EiaAXW7iqgktue5ol67bx4dkT\nuOykmQwd2K/QYZlZL+NE0AtFBNf/fSn/edcL7DaoH78+/2DeNX1UocMys17KiaCX2bStli/+dj73\nP7+GY/cezRUfehvDB5UWOiwz68WcCHqReUs38ulbn2b91hq+9r6ZnHf4FF8Sama7zImgF4gIbnps\nGd/44yIm7DaA333ycPafMLTQYZlZH+FE0MNV1zXw1TsXcseTKzl279H88IwDGVLmDmEz6zpOBD3Y\n2spqLrhhHs+urOCSd+/JZ4+b4RFCzazLORH0UK+u38Y51z7O+sparjr77Zyw39hCh2RmfZQTQQ80\nf+VmzrtuLgHceuGhHDhxWKFDMrM+zImgh/nbS+v45E1PMnxQKTd+/GD2GDW40CGZWR/nRNCD3Pn0\nKr5wx7NMH1PODee9g9FDygodkpllgBNBD/Gbucu59HcLOGTqcK4+Z7avDDKzbuNE0APc9NgyvnLn\nQo6aMYpffOztHjbazLqVE0GBXffoq3zjj4s4bp/RXPnRWfQvcRIws+7lRFBANz+e3C18/L5j+J8z\nZ1FaUlTokMwsg5wICuTe51bz1TsXcsxeo/jpWbPoV+wkYGaF4bNPAcxNB487YMIwrvyok4CZFZbP\nQN3sxdWVnH/9XCbsNoBrz30HA0tdKTOzwnIi6EYbttbw8evnUtavmBvOO9jPETCzHsE/R7tJXUMj\nF93yFOu21vDbT7yTicMHFjokMzMgzzUCSSdIelHSYkmXtrJ8kqQHJD0tab6kk/IZTyF9+8/P89iS\njfzn+/fngAkeO8jMeo68JQJJxcCVwInATOBMSTNbrPYV4PaIOAg4A/hZvuIppDvmreD6vy/l44dP\n5Z/ePqHQ4ZiZNZPPGsHBwOKIWBIRtcBtwGkt1glgSDo9FHgtj/EUxMJVFVx250IOmzaCfz9p70KH\nY2b2FvlMBOOBFTmfV6bzcl0OnC1pJXAX8OnWdiTpQknzJM1bt25dPmLNi6raei659Wl2G9iPn541\nixJfJmpmPVChz0xnAtdHxATgJODXkt4SU0RcHRGzI2L2qFGjuj3InfWtPy3i1Q3b+OGHD/QVQmbW\nY+UzEawCJuZ8npDOy3U+cDtARPwDKANG5jGmbnPPwtXc+sQKPnHUNA7bs08Uycz6qHwmgrnAdElT\nJZWSdAbPabHOcuBYAEn7kCSC3tP204aK7XV89Q8L2XfcED533IxCh2Nm1q68JYKIqAcuBu4Fnie5\nOug5Sd+UdGq62ueBCyQ9C9wKnBsRka+YussV977Ahq01fPcDB3ggOTPr8fJ6Q1lE3EXSCZw772s5\n04uAw/MZQ3d7avkmbn58OeceNoX9JwwtdDhmZh3yz9Uu1NAYXPb7hYwpL+Pz792r0OGYmXWKE0EX\n+t+nVvL861u47OR9GNzfo3eYWe/gRNBFqmrr+f59L3LgxGG874DdCx2OmVmnORF0kV89/CprttTw\nlZP3QVKhwzEz6zQngi6wfmsNV/3tFU7YdyyzpwwvdDhmZjvEiaALXPPwErbXNfDFE9xBbGa9jxPB\nLtq0rZab/rGM9x0wjmmjBhc6HDOzHeZEsIuu+/tSttU2cNExexY6FDOzneJEsAu2VNdx/aOvcvy+\nY9hrbHmhwzEz2ylOBLvgpseWsaW6nouPmV7oUMzMdpoTwU6qrW/k+keX8q7pIz2UhJn1ak4EO2nO\ns6+xtrKGf3nXHoUOxcxslzgR7ISI4JcPL2GvMeUcOd3PGjCz3s2JYCc8sng9L6yu5Px3TfVdxGbW\n6zkR7IRfPvwqIwf357QDxxU6FDOzXeZEsIOWb6jiby+t4+xDJ9G/pLjQ4ZiZ7bIOx0qWNIHkMZPv\nAsYB24GFwJ+BuyOiMa8R9jC/mbecIsFH3jGx45XNzHqBdhOBpOuA8cCfgO8Ba0meKzwDOAG4TNKl\nEfFQvgPtCeobGrlj3kqO3ms0uw8dUOhwzMy6REc1gu9HxMJW5i8Efpc+lH5S14fVM/31hbWsrazh\nzIMzU2Qzy4B2+whaSwKSpknaP11eGxGL8xVcT3PrE8sZM6Q/x+w1qtChmJl1mR16nqKkfwf2BBol\n9Y+Ij+UnrJ7n9Yrt/O2ldVx0zJ6UFLuP3cz6jo76CC4BroyIhnTW2yLiI+my+fkOrie5e8FqGgM+\nMGtCoUMxM+tSHf203QDcI+nU9PN9ku6RdB9wb35D61nuWbiavceWM3XkoEKHYmbWpTrqI7gZOAU4\nQNIc4EngA8CHIuKL3RBfj7C2spq5yzZywn5jCx2KmVmX60xj9zTgduBC4CLgx0Cmrp2877k1RMCJ\n++1e6FDMzLpcR30E1wN1wEBgVURcIOkg4BpJcyPim90QY8Hds3A1e4wcxIwxfhSlmfU9HV01dFBE\nvA1A0tMAEfE0cIqk0/IdXE+waVst/1iygQuP3MMDzJlZn9RRIrhH0r1AP+CW3AUR8Ye8RdWD3P/8\nGhoagxPdP2BmfVS7iSAiviRpCNAYEVu7KaYe5a8vrGXskDL2H++nkJlZ39RuZ7Gks4GtbSWB9C7j\nI/ISWQ9Q39DIo4vXc+SMkW4WMrM+q6OmoRHA05KeJLl0dB3JoHN7AkcB64FL8xphAT27soIt1fUc\nOcNDSphZ39VR09CPJf0UeDdwOHAAyTDUzwMfi4jl+Q+xcB56aR1FgiP29OMozazv6nCsoXR4if9L\nX5ny0MvrOGDCMIYNLC10KGZmeePR09pQUVXHsys2++H0ZtbnORG04dFX1tMYuH/AzPo8J4I2PPTS\nOsrLSjhw4rBCh2JmlledSgSSxkj6laS7088zJZ2f39AK65HF6zls2gg/e8DM+rzOnuWuJxl2elz6\n+SXgs/kIqCd4bfN2Vm7aziFTRxQ6FDOzvOtsIhgZEbcDjQARUQ80tL9J7zV36UYADp46vMCRmJnl\nX2cTwTZJI4AAkHQoUNHRRpJOkPSipMWSWr3xTNKHJS2S9JykW1pbp7vNXbqRQaXF7D22vNChmJnl\nXWefWfxvwBxgmqRHgVHAB9vbQFIxcCXwHmAlMFfSnIhYlLPOdODLwOERsUnS6J0oQ5ebt3QTsybv\n5v4BM8uETiWCiHhK0lHAXoCAFyOiroPNDgYWR8QSAEm3AacBi3LWuYDkmcib0uOs3cH4u1xldR0v\nrqn0Q2jMLDM6e9XQRcDgiHguIhYCgyV9qoPNxgMrcj6vTOflmgHMkPSopMckndDG8S+UNE/SvHXr\n1nUm5J22cNUWIuBtEz3aqJllQ2fbPi6IiM1NH9Jf8Bd0wfFLgOnA0cCZJE8+e8uF+xFxdUTMjojZ\no0bl9wavBauSYnrYaTPLis4mgmLljMOctv93NADPKmBizucJ6bxcK4E5EVEXEa+SXJY6vZMx5cX8\nlRWMHzaAEYP7FzIMM7Nu09lEcA/wG0nHSjoWuDWd1565wHRJUyWVAmeQdDjnupOkNoCkkSRNRUs6\nGVNeLFhVwQETXBsws+zobCL4EvAA8Mn09Rfg/7W3QXqvwcUkN6I9D9weEc9J+qakU9PV7gU2SFqU\n7v+LEbFhx4vRNSqq6li2oYr9nQjMLEM6e9VQI/Dz9NVpEXEXcFeLeV/LmQ6SS1P/bUf2my8LViW3\nRhww3uMLmVl2dCoRSDocuByYnG4jkvP4HvkLrfvNd0exmWVQZ28o+xXwOZLHVfbZoSUWrKxg8oiB\nDB3Yr9ChmJl1m84mgoqIuDuvkfQA81dWcNAkNwuZWbZ0trP4AUlXSHqnpFlNr7xG1s02bK1h1ebt\nvmLIzDKnszWCQ9L32TnzguSh9n1CU0fx/u4oNrOM6exVQ8fkO5BCW7CyAgn2Gz+k0KGYmXWrztYI\nkHQysC9Q1jQvIr6Zj6AK4dmVFewxchDlZe4oNrNs6eygc1cBHwE+TXLp6IdILiXtMxas2swBE9ws\nZGbZ09nO4sMi4hxgU0R8A3gnyXAQfcKaLdWs2VLj+wfMLJM6mwi2p+9VksYBdUCfGbB/wcr0jmJf\nMWRmGdTZPoI/pcNDXwE8RXLF0C/zFlU3e+61LUgwc5w7is0sezp71dC30sn/lfQnoCwiOnxmcW+x\nbOM2xg4pY2Bpp/vOzcz6jHbPfJLeHRF/lfSBVpYREb/LX2jdZ/mGKiYNH1joMMzMCqKjn8BHAX8F\nTmllWQB9IxFsrOKoGfl98pmZWU/VbiKIiK9LKgLujojbuymmbrW9toG1lTVMHuEagZllU4dXDaXP\nImj3ITS92YpNVQBMGjGowJGYmRVGZy8fvV/SFyRNlDS86ZXXyLrJsg1pInAfgZllVGcvk/lI+n5R\nzrwAev2DaZZvTBLBZCcCM8uozl4+OjXfgRTK8g3bKC8rYZgfRmNmGbUjg87tB8yk+aBzN+YjqO60\nYtN2Ju42EEmFDsXMrCA6+8zirwNHkySCu4ATgUeAXp8IXtu8nYluFjKzDOtsZ/EHgWOB1RFxHvA2\noE8MzLNq03bGDxtQ6DDMzAqm04POpZeR1ksaAqwFJuYvrO5Rsb2Oypp6JwIzy7TO9hHMSweduwZ4\nEtgK/CNvUXWT1zYng6qO382JwMyyq6Oxhq4EbomIT6WzrpJ0DzAkIubnPbo8W7UpSQTjXCMwswzr\nqEbwEvDfknYHbgdujYin8x9W91jVVCNwIjCzDGu3jyAifhwR7yQZfG4DcK2kFyR9XVKvf0LZa5u3\nU1pSxIhBpYUOxcysYDrVWRwRyyLiexFxEHAmcDrwfF4j6wYrNydXDBUV+R4CM8uuzj68vkTSKZJu\nBu4GXgTe8oyC3mbVpu2MG1bW8YpmZn1YR53F7yGpAZwEPAHcBlwYEdu6Iba8e23zdo7ey88hMLNs\n66iz+MvALcDnI2JTN8TTbWrqk+cQjB/mu4rNLNs6ejDNu7srkO62uqIawE1DZpZ5nb2zuM9puofA\nN5OZWdZlNhGs9D0EZmZAhhPBa5u3I8HuQ50IzCzbMpsIVm3azujy/pSWZPZPYGYGZDkRbN7uMYbM\nzMhwInhts59DYGYGGU0EEcFrFdWuEZiZkedEIOkESS9KWizp0nbW+ydJIWl2PuNpsqmqjtr6RsYO\n8T0EZmZ5SwSSioErSZ5vPBM4U9LMVtYrBz4DPJ6vWFpqupls7FAnAjOzfNYIDgYWR8SSiKglGafo\ntFbW+xbwPaA6j7E0s6YyOdSYIf2765BmZj1WPhPBeGBFzueV6bw3SJoFTIyIP7e3I0kXSponad66\ndet2ObA1FU2JwDUCM7OCdRZLKgJ+AHy+o3Uj4uqImB0Rs0eN2vXRQtdsqQFgdLkTgZlZPhPBKmBi\nzucJ6bwm5cB+wIOSlgKHAnO6o8N4TWU1IwaV+mYyMzPymwjmAtMlTZVUCpwBzGlaGBEVETEyIqZE\nxBTgMeDUiJiXx5iApGlotJuFzMyAPCaCiKgHLgbuJXms5e0R8Zykb0o6NV/H7Yw1ldWMdUexmRnQ\n8YNpdklE3AXc1WLe19pY9+h8xpJrdUUN+40b2l2HMzPr0TLXSF7X0MiGbTW+YsjMLJW5RLB+aw0R\nMNpNQ2ZmQAYTweaqOgCGDywtcCRmZj1D5hLB1pp6AAaX5bV7xMys18heIqhOE0F/JwIzM8hgIqhM\nawTlZf0KHImZWc+QvURQnfQRlLtpyMwMyGAicNOQmVlz2UsENfUUCQaWFhc6FDOzHiFziaCyup7B\n/UuQVOhQzMx6hEwmAncUm5m9KXOJYGtNnfsHzMxyZDAR1PuKITOzHJlLBJXV9b6r2MwsR+YSwda0\ns9jMzBKZSwSVNe4sNjPLlblEsLXafQRmZrkylQjqGhrZXtfgpiEzsxyZSgTb3hhwzonAzKxJphJB\npccZMjN7i0wmAtcIzMzelKlE8MbTyfr7qiEzsyYZSwTJswh8Q5mZ2ZsylQi21TQAMLi/h6A2M2uS\nqURQVZs0DQ0odY3AzKxJxhJBUiMY5IfSmJm9IZOJYKBrBGZmb8hUIthWU0+/YlFakqlim5m1K1Nn\nxKraBgb0c7OQmVmuTCWCbTX1DPJdxWZmzWQqEVTVNTDQHcVmZs1kKxG4RmBm9haZSgTb3EdgZvYW\nmUoEVbWuEZiZtZSxROA+AjOzlrKVCGoaGOSbyczMmslUIthWW88A1wjMzJrJTCKICKpqGxjkkUfN\nzJrJTCKobWikoTE8zpCZWYj2xMoAAAmgSURBVAt5TQSSTpD0oqTFki5tZfm/SVokab6kv0ianK9Y\nqmqaBpxzjcDMLFfeEoGkYuBK4ERgJnCmpJktVnsamB0RBwC/Bf4rX/FsS59F4M5iM7Pm8lkjOBhY\nHBFLIqIWuA04LXeFiHggIqrSj48BE/IVzBtDULuPwMysmXwmgvHAipzPK9N5bTkfuDtfwbz5UBrX\nCMzMcvWIs6Kks4HZwFFtLL8QuBBg0qRJO3WMqpqmx1S6RmBmliufNYJVwMSczxPSec1IOg64DDg1\nImpa21FEXB0RsyNi9qhRo3YqmG2uEZiZtSqfiWAuMF3SVEmlwBnAnNwVJB0E/IIkCazNYyxvPLje\nfQRmZs3lLRFERD1wMXAv8Dxwe0Q8J+mbkk5NV7sCGAzcIekZSXPa2N0u21bjGoGZWWvyelaMiLuA\nu1rM+1rO9HH5PH6uphqB+wjMzJrLzJ3Fk4YP5IR9x/qGMjOzFjLTTvLefcfy3n3HFjoMM7MeJzM1\nAjMza50TgZlZxjkRmJllnBOBmVnGORGYmWWcE4GZWcY5EZiZZZwTgZlZxikiCh3DDpG0Dli2k5uP\nBNZ3YTg9lcvZ92SlrC5n/kyOiFaHb+51iWBXSJoXEbMLHUe+uZx9T1bK6nIWhpuGzMwyzonAzCzj\nspYIri50AN3E5ex7slJWl7MAMtVHYGZmb5W1GoGZmbXgRGBmlnGZSQSSTpD0oqTFki4tdDxdSdJS\nSQvS5z7PS+cNl/R/kl5O33crdJw7StK1ktZKWpgzr9VyKfGT9PudL2lW4SLfMW2U83JJq9Lv9BlJ\nJ+Us+3JazhclHV+YqHecpImSHpC0SNJzkj6Tzu9T32k75ey532lE9PkXUAy8AuwBlALPAjMLHVcX\nlm8pMLLFvP8CLk2nLwW+V+g4d6JcRwKzgIUdlQs4CbgbEHAo8Hih49/Fcl4OfKGVdWem/377A1PT\nf9fFhS5DJ8u5OzArnS4HXkrL06e+03bK2WO/06zUCA4GFkfEkoioBW4DTitwTPl2GnBDOn0DcHoB\nY9kpEfEQsLHF7LbKdRpwYyQeA4ZJ2r17It01bZSzLacBt0VETUS8Ciwm+ffd40XE6xHxVDpdCTwP\njKePfaftlLMtBf9Os5IIxgMrcj6vpP0vprcJ4D5JT0q6MJ03JiJeT6dXA2MKE1qXa6tcffE7vjht\nErk2p2mvT5RT0hTgIOBx+vB32qKc0EO/06wkgr7uiIiYBZwIXCTpyNyFkdQ/+9x1wn21XKmfA9OA\nA4HXge8XNpyuI2kw8L/AZyNiS+6yvvSdtlLOHvudZiURrAIm5nyekM7rEyJiVfq+Fvg9SbVyTVM1\nOn1fW7gIu1Rb5epT33FErImIhohoBK7hzaaCXl1OSf1ITo43R8Tv0tl97jttrZw9+TvNSiKYC0yX\nNFVSKXAGMKfAMXUJSYMklTdNA+8FFpKU75/T1f4Z+ENhIuxybZVrDnBOeqXJoUBFTnNDr9OiLfz9\nJN8pJOU8Q1J/SVOB6cAT3R3fzpAk4FfA8xHxg5xFfeo7baucPfo7LXQPe3e9SK5AeImkR/6yQsfT\nheXag+SKg2eB55rKBowA/gK8DNwPDC90rDtRtltJqtB1JO2m57dVLpIrS65Mv98FwOxCx7+L5fx1\nWo75JCeK3XPWvywt54vAiYWOfwfKeQRJs8984Jn0dVJf+07bKWeP/U49xISZWcZlpWnIzMza4ERg\nZpZxTgRmZhnnRGBmlnFOBGZmGedEYHknKSR9P+fzFyRd3kX7vl7SB7tiXx0c50OSnpf0QCvLZki6\nKx098ylJt0vq1UN6SDpd0sxCx2Hdw4nAukMN8AFJIwsdSC5JJTuw+vnABRFxTIt9lAF/Bn4eEdMj\nGerjZ8Corou0IE4nGRXTMsCJwLpDPckzWj/XckHLX/SStqbvR0v6m6Q/SFoi6buSPirpCSXPXpiW\ns5vjJM2T9JKk96XbF0u6QtLcdJCvf83Z78OS5gCLWonnzHT/CyV9L533NZKbhH4l6YoWm5wF/CMi\n/tg0IyIejIiFksokXZfu72lJx6T7O1fSnUrG3l8q6WJJ/5au85ik4el6D0r6cTp2/UJJB6fzh6fb\nz0/XPyCdf3k6mNmD6d/skpxynZ3+7Z6R9AtJxU1/b0nflvRsuq8xkg4DTgWuSNefJukSJePrz5d0\nW2e+dOtFCn0Xnl99/wVsBYaQPDdhKPAF4PJ02fXAB3PXTd+PBjaTjO3en2TslW+kyz4D/Chn+3tI\nftRMJ7kztwy4EPhKuk5/YB7JWO9HA9uAqa3EOQ5YTvJrvgT4K3B6uuxBWrmzFfgB8Jk2yv154Np0\neu9032XAuSRDDZenx6oAPpGu90OSQcqajnlNOn0k6fMKgP8Bvp5Ovxt4Jp2+HPh7Wt6RwAagH7AP\n8EegX7rez4Bz0ukATkmn/yvnb9bye3kN6J9ODyv0vym/uvblGoF1i0hGX7wRuKSjdXPMjWRs9xqS\n2+/vS+cvAKbkrHd7RDRGxMvAEpKT7ntJxql5hmQI4BEkiQLgiUjGfW/pHcCDEbEuIuqBm0lOwDvr\nCOAmgIh4AVgGzEiXPRARlRGxjiQRNNUoWpbt1nT7h4Ahkoal+/11Ov+vwAhJQ9L1/xzJuPbrSQZv\nGwMcC7wdmJv+PY4lGZoEoBb4Uzr9ZItj55oP3CzpbJIanvUhO9JGararfgQ8BVyXM6+etIlSUhHJ\nE+Sa1ORMN+Z8bqT5v92W46QEyTg1n46Ie3MXSDqapEbQVZ4DjtqJ7XalbJ3db0O6LwE3RMSXW1m/\nLiKixfqtOZkkKZ4CXCZp/zRZWh/gGoF1m4jYCNxO0vHaZCnJr1VI2qX77cSuPySpKO032INk4K57\ngU8qGQ646cqeQR3s5wngKEkj0zb0M4G/dbDNLcBhkk5umiHpSEn7AQ8DH206PjApjW1HfCTd/giS\n0TcrWuz3aGB9tBjXv4W/AB+UNDrdZrikyR0ct5Kk6aopQU+MiAeAL5E07w3ewXJYD+YagXW37wMX\n53y+BviDpGdJ2vp35tf6cpKT+BCStvZqSb8kaeZ4Kh0WeB0dPK4zIl6XdCnwAMmv6D9HRLvDd0fE\n9rSD+keSfkQyguh8kn6MnwE/l7SApOZzbkTUJOF0WrWkp0kS5MfTeZcD10qaD1Tx5hDObcW4SNJX\nSJ5iV5TGeBFJU1VbbgOuSTuczyDpKB9K8nf5SURs3pFCWM/m0UfNeihJD5I87HxeoWOxvs1NQ2Zm\nGecagZlZxrlGYGaWcU4EZmYZ50RgZpZxTgRmZhnnRGBmlnH/H3IrT4Ps7N6rAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgtMEyyJb6BZ",
        "colab_type": "text"
      },
      "source": [
        "## KNN Classifier\n",
        "\n",
        "Vraag: met grid_search.best_estimator_ krijg ik als het beste resultaat k=33, maar als ik k=25 invul krijg ik een hoger resultaat voor test en train. \n",
        "\n",
        "vgm snap ik dit nu wel...Soms grid_search.best_estimator_ ander resultaat voor beste k dan clf.n_neigbors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N-xw5GccCNu",
        "colab_type": "code",
        "outputId": "ba4f58e4-7c8d-41f9-d2fa-9ac0f340905e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
        "# Specify the classifier\n",
        "knn = neighbors.KNeighborsClassifier()\n",
        "parameters = {\"n_neighbors\": list(range(1, 51, 2))}\n",
        "cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "grid_search = model_selection.GridSearchCV(knn, parameters, cv=cv_10fold, scoring='accuracy')\n",
        "grid_search.fit(X_train_pca, Y_train)\n",
        "# Show the complete results of the cross validation\n",
        "display(pd.DataFrame(grid_search.cv_results_))\n",
        "\n",
        "\n",
        "# # Fit kNN\n",
        "# Get resulting classifier\n",
        "print(grid_search.best_estimator_)\n",
        "#print(f'Best classifier: k={clf.n_neighbors}')\n",
        "clf = neighbors.KNeighborsClassifier(n_neighbors=21)\n",
        "clf.fit(X_train_pca, Y_train)\n",
        "score_train = clf.score(X_train_pca, Y_train)\n",
        "score_test = clf.score(X_test_pca, Y_test)\n",
        "\n",
        "# Get the accuracy\n",
        "y_pred = clf.predict(X_train_pca)\n",
        "acc_train=metrics.accuracy_score(Y_train, y_pred)\n",
        "y_pred = clf.predict(X_test_pca)\n",
        "acc_test=metrics.accuracy_score(Y_test, y_pred)\n",
        "\n",
        "# Print result\n",
        "print(f\"Training result: {score_train}\")\n",
        "print(f\"Test result: {score_test}\")\n",
        "print(f\"Accuracy:\")\n",
        "print(f\"Training result: {acc_train}\")\n",
        "print(f\"Test result: {acc_test}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_n_neighbors</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>split5_test_score</th>\n",
              "      <th>split6_test_score</th>\n",
              "      <th>split7_test_score</th>\n",
              "      <th>split8_test_score</th>\n",
              "      <th>split9_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>0.003516</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 1}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.809524</td>\n",
              "      <td>0.595238</td>\n",
              "      <td>0.690532</td>\n",
              "      <td>0.079030</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001362</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.003362</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>3</td>\n",
              "      <td>{'n_neighbors': 3}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.595238</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.721152</td>\n",
              "      <td>0.071976</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.003239</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>5</td>\n",
              "      <td>{'n_neighbors': 5}</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.739978</td>\n",
              "      <td>0.056398</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.003274</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>7</td>\n",
              "      <td>{'n_neighbors': 7}</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.765615</td>\n",
              "      <td>0.050349</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001394</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.003385</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>9</td>\n",
              "      <td>{'n_neighbors': 9}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.765615</td>\n",
              "      <td>0.052503</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.001369</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>11</td>\n",
              "      <td>{'n_neighbors': 11}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.770377</td>\n",
              "      <td>0.047710</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.001360</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.003614</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>13</td>\n",
              "      <td>{'n_neighbors': 13}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.767996</td>\n",
              "      <td>0.049548</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001343</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.003455</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>15</td>\n",
              "      <td>{'n_neighbors': 15}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.772757</td>\n",
              "      <td>0.058219</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.003437</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>17</td>\n",
              "      <td>{'n_neighbors': 17}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.779845</td>\n",
              "      <td>0.054036</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.003421</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>19</td>\n",
              "      <td>{'n_neighbors': 19}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.761905</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.789147</td>\n",
              "      <td>0.055093</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.001362</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.003649</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>21</td>\n",
              "      <td>{'n_neighbors': 21}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.775138</td>\n",
              "      <td>0.056139</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.004071</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>23</td>\n",
              "      <td>{'n_neighbors': 23}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.775138</td>\n",
              "      <td>0.057095</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.001455</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.003850</td>\n",
              "      <td>0.000584</td>\n",
              "      <td>25</td>\n",
              "      <td>{'n_neighbors': 25}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.779790</td>\n",
              "      <td>0.057225</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.001347</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.003568</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>27</td>\n",
              "      <td>{'n_neighbors': 27}</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.777464</td>\n",
              "      <td>0.054297</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.001374</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.003756</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>29</td>\n",
              "      <td>{'n_neighbors': 29}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.768106</td>\n",
              "      <td>0.047895</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.001341</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.003815</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>31</td>\n",
              "      <td>{'n_neighbors': 31}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.768051</td>\n",
              "      <td>0.056603</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.001358</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.003811</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>33</td>\n",
              "      <td>{'n_neighbors': 33}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.697674</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.772813</td>\n",
              "      <td>0.064042</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.001438</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.004454</td>\n",
              "      <td>0.001008</td>\n",
              "      <td>35</td>\n",
              "      <td>{'n_neighbors': 35}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.772813</td>\n",
              "      <td>0.064881</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.003937</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>37</td>\n",
              "      <td>{'n_neighbors': 37}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.770487</td>\n",
              "      <td>0.063772</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.003633</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>39</td>\n",
              "      <td>{'n_neighbors': 39}</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.768106</td>\n",
              "      <td>0.063440</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.001341</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.003826</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>41</td>\n",
              "      <td>{'n_neighbors': 41}</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.880952</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.775138</td>\n",
              "      <td>0.069873</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.001391</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.004001</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>43</td>\n",
              "      <td>{'n_neighbors': 43}</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.738095</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.763455</td>\n",
              "      <td>0.065004</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.001334</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.003782</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>45</td>\n",
              "      <td>{'n_neighbors': 45}</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.756423</td>\n",
              "      <td>0.062320</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>47</td>\n",
              "      <td>{'n_neighbors': 47}</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.720930</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.761074</td>\n",
              "      <td>0.067122</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.001315</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.003728</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>49</td>\n",
              "      <td>{'n_neighbors': 49}</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.674419</td>\n",
              "      <td>0.813953</td>\n",
              "      <td>0.837209</td>\n",
              "      <td>0.744186</td>\n",
              "      <td>0.767442</td>\n",
              "      <td>0.790698</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.690476</td>\n",
              "      <td>0.761019</td>\n",
              "      <td>0.054759</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "0        0.001997      0.001711  ...        0.079030               25\n",
              "1        0.001362      0.000085  ...        0.071976               24\n",
              "2        0.001319      0.000010  ...        0.056398               23\n",
              "3        0.001323      0.000024  ...        0.050349               17\n",
              "4        0.001394      0.000155  ...        0.052503               17\n",
              "5        0.001369      0.000055  ...        0.047710               12\n",
              "6        0.001360      0.000030  ...        0.049548               16\n",
              "7        0.001343      0.000049  ...        0.058219               10\n",
              "8        0.001312      0.000013  ...        0.054036                2\n",
              "9        0.001328      0.000051  ...        0.055093                1\n",
              "10       0.001362      0.000091  ...        0.056139                5\n",
              "11       0.001454      0.000120  ...        0.057095                5\n",
              "12       0.001455      0.000238  ...        0.057225                3\n",
              "13       0.001347      0.000024  ...        0.054297                4\n",
              "14       0.001374      0.000070  ...        0.047895               13\n",
              "15       0.001341      0.000005  ...        0.056603               15\n",
              "16       0.001358      0.000030  ...        0.064042                8\n",
              "17       0.001438      0.000232  ...        0.064881                8\n",
              "18       0.001365      0.000043  ...        0.063772               11\n",
              "19       0.001317      0.000020  ...        0.063440               13\n",
              "20       0.001341      0.000033  ...        0.069873                5\n",
              "21       0.001391      0.000052  ...        0.065004               19\n",
              "22       0.001334      0.000020  ...        0.062320               22\n",
              "23       0.001323      0.000024  ...        0.067122               20\n",
              "24       0.001315      0.000010  ...        0.054759               21\n",
              "\n",
              "[25 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=19, p=2,\n",
            "                     weights='uniform')\n",
            "Training result: 0.8056206088992974\n",
            "Test result: 0.7873831775700935\n",
            "Accuracy:\n",
            "Training result: 0.8056206088992974\n",
            "Test result: 0.7873831775700935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO6550RrcGRd",
        "colab_type": "text"
      },
      "source": [
        "## KNN with Crossvalidation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1baCzMI2cKVq",
        "colab_type": "code",
        "outputId": "2fb5321c-0e27-44db-925d-089489000e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        }
      },
      "source": [
        "# Create a 20 fold stratified CV iterator\n",
        "cv_20fold = model_selection.StratifiedKFold(n_splits=20)\n",
        "results = []\n",
        "results_acc = []\n",
        "best_n_neighbors = []\n",
        "\n",
        "# Loop over the folds\n",
        "for validation_index, test_index in cv_20fold.split(X_train_pca, Y_train):\n",
        "    # Split the data properly\n",
        "    X_validation = X_train_pca[validation_index]\n",
        "    y_validation = np.array(Y_train)[validation_index]\n",
        "    \n",
        "    X_testKNN = X_train_pca[test_index]\n",
        "    y_testKNN = np.array(Y_train)[test_index]\n",
        "    \n",
        "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
        "    # Same as above\n",
        "    parameters = {\"n_neighbors\": list(range(1, 26, 2))}\n",
        "    knn = neighbors.KNeighborsClassifier()\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=10)\n",
        "    grid_search = model_selection.GridSearchCV(knn, parameters, cv=cv_10fold, scoring='roc_auc')\n",
        "    grid_search.fit(X_validation, y_validation)\n",
        "    \n",
        "    # Get resulting classifier\n",
        "    clf = grid_search.best_estimator_\n",
        "    print(f'Best classifier: k={clf.n_neighbors}')\n",
        "    best_n_neighbors.append(clf.n_neighbors)\n",
        "    \n",
        "    # Test the classifier on the test data\n",
        "    probabilities = clf.predict_proba(X_testKNN)\n",
        "    scores = probabilities[:, 1]\n",
        "\n",
        "    # Get the accuracy\n",
        "    y_pred = clf.predict(X_validation)\n",
        "    accuracy=metrics.accuracy_score(y_validation, y_pred)\n",
        "    results_acc.append({'acc': accuracy,'set': 'validation'})\n",
        "    y_pred = clf.predict(X_testKNN)\n",
        "    accuracy = metrics.accuracy_score(y_testKNN, y_pred)\n",
        "    results_acc.append({'acc': accuracy,'set': 'test'})\n",
        "    \n",
        "    # Get the auc\n",
        "    auc = metrics.roc_auc_score(y_testKNN, scores)\n",
        "    results.append({'auc': auc,'k': clf.n_neighbors,'set': 'test'})\n",
        "    \n",
        "    # Test the classifier on the validation data\n",
        "    probabilities_validation = clf.predict_proba(X_validation)\n",
        "    scores_validation = probabilities_validation[:, 1]\n",
        "    \n",
        "    # Get the auc\n",
        "    auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
        "    results.append({\n",
        "        'auc': auc_validation,\n",
        "        'k': clf.n_neighbors,\n",
        "        'set': 'validation'\n",
        "    })\n",
        "    \n",
        "# Create results dataframe and plot it\n",
        "results = pd.DataFrame(results)\n",
        "plt.show()\n",
        "seaborn.boxplot(y='auc', x='set', data=results)\n",
        "plt.show()\n",
        "results_acc = pd.DataFrame(results_acc)\n",
        "seaborn.boxplot(y='acc', x='set', data=results_acc)\n",
        "\n",
        "optimal_n = int(np.median(best_n_neighbors))\n",
        "print(f\"The optimal N={optimal_n}\")\n",
        "\n",
        "\n",
        "print(clf.score(X_test_pca, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best classifier: k=17\n",
            "Best classifier: k=17\n",
            "Best classifier: k=17\n",
            "Best classifier: k=17\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=13\n",
            "Best classifier: k=15\n",
            "Best classifier: k=19\n",
            "Best classifier: k=15\n",
            "Best classifier: k=15\n",
            "Best classifier: k=19\n",
            "Best classifier: k=19\n",
            "Best classifier: k=17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElNBoQkqcmAkCQ1GRCSpCYDQpLU9P8BO83inqTYcNIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The optimal N=15\n",
            "0.7920560747663551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9TA2vNS7f8v",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machine (SVM) Classifier \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4v-fbsF7ueo",
        "colab_type": "code",
        "outputId": "de941cf0-7bc4-4b38-8470-18a6b2637343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Construct classifiers and corresponding kernel (comment the ones that we do not want to use)\n",
        "\n",
        "# Linear kernel:\n",
        "svmlin = SVC(kernel='linear', gamma='scale')\n",
        "# Radial Basis Function (RBF) kernel:\n",
        "svmrbf = SVC(kernel='rbf', gamma='scale')\n",
        "# Polynomial kernel:\n",
        "svmpoly = SVC(kernel='poly', degree=3, gamma='scale')\n",
        "\n",
        "# Other kernels that are an option:\n",
        "# - change gamma\n",
        "# - first use RBF sampler, then linear kernel\n",
        "# - manually constructed kernel function?\n",
        "# - precomputed kernel\n",
        "# - sigmoid kernel\n",
        "\n",
        "clsfs = [svmlin, svmpoly, svmrbf]\n",
        "\n",
        "# Create lists of datasets to loop over (dit klopt nog niet, aanpassen)\n",
        "\n",
        "Xs = X\n",
        "Ys = Y\n",
        "\n",
        "# First make plot without classifiers:\n",
        "num = 0\n",
        "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
        "for X, Y in zip(Xs, Ys):\n",
        "    ax = fig.add_subplot(7, 3, num + 1)\n",
        "    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
        "        s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "    num += 1\n",
        "    \n",
        "# Now use the classifiers on all datasets\n",
        "for clf in clsfs:\n",
        "    for X, Y in zip(Xs, Ys):\n",
        "        clf.fit(X, Y)\n",
        "        ax = fig.add_subplot(7, 3, num + 1)\n",
        "        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
        "            s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "        colorplot(clf, ax, X[:, 0], X[:, 1])\n",
        "        y_pred = clf.predict(X)\n",
        "        t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n",
        "        ax.set_title(t)\n",
        "        num += 1\n",
        "\n",
        "# Important hyperparameters:\n",
        "# - degree of the kernel\n",
        "# - coef0s\n",
        "# - slacks\n",
        "\n",
        "# Tune hyperparameters\n",
        "degrees = [1, 3, 5]\n",
        "coef0s = [0.01, 0.5, 1]\n",
        "slacks = [0.01, 0.5, 1]\n",
        "\n",
        "clsfs = list()\n",
        "for degree in degrees:\n",
        "    for coef0 in coef0s:\n",
        "        for slack in slacks:\n",
        "            clsfs.append(SVC(kernel='poly', degree=degree, coef0=coef0, C=slack, gamma='scale'))\n",
        "\n",
        "# First make plot without classifiers:\n",
        "num = 0\n",
        "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
        "for X, Y in zip(Xs, Ys):\n",
        "    ax = fig.add_subplot(len(clsfs) + 1, 3, num + 1)\n",
        "    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
        "        s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "    num += 1\n",
        "    \n",
        "# Now use the classifiers on all datasets\n",
        "for clf in clsfs:\n",
        "    for X, Y in zip(Xs, Ys):\n",
        "        clf.fit(X, Y)\n",
        "        ax = fig.add_subplot(len(clsfs) + 1, 3, num + 1)\n",
        "        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
        "            s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "        colorplot(clf, ax, X[:, 0], X[:, 1])\n",
        "        y_pred = clf.predict(X)\n",
        "        t = f\"degree: {clf.degree}, coef0: {clf.coef0}, C: {clf.C}. \"\n",
        "        t = t + (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n",
        "        ax.set_title(t)\n",
        "        num += 1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-24e44dba83ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvmlin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Radial Basis Function (RBF) kernel:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msvmrbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Polynomial kernel:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvmpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SVC' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3VeeVBm_Bbo",
        "colab_type": "text"
      },
      "source": [
        "**Random Forrest Classifier**\n",
        "\n",
        "The Random Forrest Classifier has 19 different hyperparameters:\n",
        "- n_estimators: number of trees (to much --> overfitting)\n",
        "- criterion: How to measure the quality of a split (gini) (tree-specific)\n",
        "- max_depth: depth of the tree\n",
        "- min_samples_split: minimum number of samples required to split an internal node\n",
        "- min_samples_leaf: number of samples required to be at a node\n",
        "- min_weight_fraction_leaf: sum total of the weigths at a node.\n",
        "- max_features: number of features to consider\n",
        "- max_leaf_nodes: \n",
        "- min_impurity_decrease: node will split if impurity decreases with this value\n",
        "- min_impurity_split: A node will split if its impurity is above the threshold, otherwise it is a leaf\n",
        "- bootstrap: use or not (T/F) (To improve accuracy by creating samples)\n",
        "if T --> max_samples: number of samples to train each base estimator\n",
        "- oob_score: use out-of-bag samples\n",
        "- n_jobs: \n",
        "- random_state: \n",
        "- verbose:\n",
        "- warm_start:\n",
        "- class_weight: If one class is more important\n",
        "- ccp_alpha: x\n",
        "\n",
        "--> Misschien toevoegen: een tijd segment om te kijken of het niet te lang duurt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkxtQjuq-_i0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e04de75-5cf7-44bf-cc5c-178f36cc6975"
      },
      "source": [
        "#Hyperparameters tuning:\n",
        "n_tree = [10,25,50,100]  # n_estimators\n",
        "boot = ['True','False']  # Bootstrapping\n",
        "criterion = ['gini','entropy'] # measure of quality\n",
        "max_depth = [range(5, 16)] # Depth of the tree\n",
        "min_samples_split = [range(2,7)] # prevends overfitting\n",
        "min_samples_leaf = [1,2] # prevends overfitting\n",
        "classweight = [{0: 1, 1: 0.001},{0: 1, 1: 1},\n",
        "               {0: 1, 1: 10},{0: 1, 1: 100}]\n",
        "\n",
        "\n",
        "clsfs = [n_tree, boot, criterion, max_depth, min_samples_split, \n",
        "         min_samples_leaf, classweight]\n",
        "\n",
        "\n",
        "# Y_test = validation set, niet de test set\n",
        "for clf in clsfs:\n",
        "  for hype_par in clf:\n",
        "    for X, Y in zip(Xtrain,Ytrain):\n",
        "      clf.fit(X, Y)\n",
        "      y_pred_train = clf.predict(X_train_pca)\n",
        "      acc_train = metrics.accuracy_score(Y_train, y_pred)\n",
        "      y_pred_test = clf.predict(X_test_pca)\n",
        "      acc_test = metrics.accuracy_score(Y_test, y_pred)\n",
        "      acc_test_clsfs.append(acc_test)\n",
        "    maxpos = acc.index(max(acc_test_clsfs))\n",
        "  # hier moet nog dat het die plek van deze classifier is \n",
        "\n",
        "clsfs_final = [RandomForestClassifier(n_estimators = n_tree(n_tree[maxpos]))]\n",
        "\n",
        "# First make plot without classifiers:\n",
        "num = 0\n",
        "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
        "for X, Y in zip(Xs, Ys):\n",
        "    ax = fig.add_subplot(7, 3, num + 1)\n",
        "    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
        "        s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "    num += 1\n",
        "    \n",
        "# Now use the classifiers on all datasets\n",
        "for clf in clsfs:\n",
        "    for X, Y in zip(Xs, Ys):\n",
        "        clf.fit(X, Y)\n",
        "        ax = fig.add_subplot(7, 3, num + 1)\n",
        "        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
        "            s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "        colorplot(clf, ax, X[:, 0], X[:, 1])\n",
        "        y_pred = clf.predict(X)\n",
        "        t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n",
        "        ax.set_title(t)\n",
        "        num += 1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF9D8tov5dlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}